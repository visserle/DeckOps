"""Import Markdown files back into Anki."""

import logging
from dataclasses import dataclass, field
from pathlib import Path

from deckops.anki_client import invoke
from deckops.config import NOTE_SEPARATOR, SUPPORTED_NOTE_TYPES
from deckops.markdown_converter import MarkdownToHTML
from deckops.markdown_helpers import (
    ParsedNote,
    _note_identifier,
    extract_deck_id,
    parse_note_block,
    validate_note,
)

logger = logging.getLogger(__name__)


@dataclass
class NoteInfo:
    """Cached information about a note from Anki."""

    note_id: int
    fields: dict[str, str]
    card_ids: list[int]
    model_name: str


@dataclass
class FileImportResult:
    """Result of importing an entire file."""

    file_path: Path
    total_notes: int
    updated: int
    created: int
    deleted: int
    skipped: int
    errors: list[str] = field(default_factory=list)


@dataclass
class AnkiCache:
    """Pre-fetched Anki data to avoid redundant API calls.

    Built once in import_collection() for all files, or on-demand
    in import_file() for single-file imports.
    """

    notes: dict[int, NoteInfo] = field(default_factory=dict)
    cards: dict[int, dict] = field(default_factory=dict)
    deck_note_ids: dict[str, set[int]] = field(default_factory=dict)


def _fetch_notes_by_id(note_ids: list[int]) -> dict[int, NoteInfo]:
    """Fetch notes from AnkiConnect and build NoteInfo cache.

    Validates that all returned notes have DeckOps templates.
    """
    result: dict[int, NoteInfo] = {}
    for note in invoke("notesInfo", notes=note_ids):
        if not note:
            continue
        model = note.get("modelName")
        if model and model not in SUPPORTED_NOTE_TYPES:
            raise ValueError(
                f"Safety check failed: Note {note['noteId']} has template "
                f"'{model}' but expected a DeckOps template. "
                f"DeckOps will never modify notes with non-DeckOps templates."
            )
        result[note["noteId"]] = NoteInfo(
            note_id=note["noteId"],
            fields={name: info["value"] for name, info in note["fields"].items()},
            card_ids=note.get("cards", []),
            model_name=model,
        )
    return result


def _fetch_cards_cache() -> tuple[dict[int, dict], dict[str, set[int]]]:
    """Fetch all DeckOps cards from AnkiConnect.

    Returns (cards_by_id, note_ids_by_deck).
    """
    note_type_query = " OR ".join(f"note:{nt}" for nt in SUPPORTED_NOTE_TYPES)
    all_card_ids = invoke("findCards", query=note_type_query)
    cards_by_id: dict[int, dict] = {}
    note_ids_by_deck: dict[str, set[int]] = {}
    if all_card_ids:
        for card in invoke("cardsInfo", cards=all_card_ids):
            cards_by_id[card["cardId"]] = card
            note_ids_by_deck.setdefault(card["deckName"], set()).add(card["note"])
    return cards_by_id, note_ids_by_deck


def parse_deck_id(file_path: Path) -> int | None:
    """Extract deck_id from the first line of a markdown file."""
    content = file_path.read_text(encoding="utf-8")
    deck_id, _ = extract_deck_id(content)
    return deck_id


def write_deck_id_to_file(file_path: Path, deck_id: int) -> None:
    """Write or replace the deck_id on the first line of a markdown file."""
    content = file_path.read_text(encoding="utf-8")
    _, remaining = extract_deck_id(content)
    new_first_line = f"<!-- deck_id: {deck_id} -->\n"
    file_path.write_text(new_first_line + remaining, encoding="utf-8")


def parse_markdown_file(file_path: Path) -> list[ParsedNote]:
    """Parse a markdown file into a list of ParsedNote objects."""
    content = file_path.read_text(encoding="utf-8")
    _, content = extract_deck_id(content)
    blocks = content.split(NOTE_SEPARATOR)
    return [parse_note_block(block) for block in blocks if block.strip()]


def convert_fields_to_html(
    fields: dict[str, str],
    converter: MarkdownToHTML,
) -> dict[str, str]:
    """Convert all field values from markdown to HTML."""
    return {name: converter.convert(content) for name, content in fields.items()}


def _batch_write_ids_to_file(
    file_path: Path,
    id_assignments: list[tuple[ParsedNote, int]],
) -> None:
    """Write note_ids back into the markdown file in a single pass.

    Handles two cases:
    - New notes (no old ID): prepend the ID comment before the first field line
    - Stale notes (had old ID deleted from Anki): replace the old ID with new one
    """
    if not id_assignments:
        return

    # VALIDATION: Check for duplicate first lines among new notes
    # This would cause only the first note to get an ID, orphaning the others
    new_notes_first_lines: dict[str, list[str]] = {}
    for parsed_note, id_value in id_assignments:
        if parsed_note.note_id is None:  # New note without existing ID
            first_line = parsed_note.raw_content.strip().split("\n")[0]
            if first_line not in new_notes_first_lines:
                new_notes_first_lines[first_line] = []
            new_notes_first_lines[first_line].append(_note_identifier(parsed_note))

    # Check for duplicates
    duplicates = {
        line: identifiers
        for line, identifiers in new_notes_first_lines.items()
        if len(identifiers) > 1
    }
    if duplicates:
        error_msg = f"ERROR: Duplicate first lines detected in {file_path.name}:\n"
        for first_line, identifiers in duplicates.items():
            notes_list = ", ".join(identifiers)
            error_msg += f"  '{first_line[:60]}...' in notes: {notes_list}\n"
        error_msg += (
            "Cannot safely assign IDs. Please ensure each note has a unique first line."
        )
        raise ValueError(error_msg)

    content = file_path.read_text(encoding="utf-8")

    replacements: list[tuple[str, str]] = []
    for parsed_note, id_value in id_assignments:
        new_id_comment = f"<!-- note_id: {id_value} -->"

        if parsed_note.note_id is not None:
            # Stale note: replace old ID comment with new one
            old_id_comment = f"<!-- note_id: {parsed_note.note_id} -->"
            replacements.append((old_id_comment, new_id_comment))
        else:
            # New note: prepend ID comment before first content line
            first_line = parsed_note.raw_content.strip().split("\n")[0]
            replacements.append((first_line, new_id_comment + "\n" + first_line))

    for old_text, new_text in replacements:
        content = content.replace(old_text, new_text, 1)

    file_path.write_text(content, encoding="utf-8")


def _import_existing_notes(
    existing_notes: list[tuple[ParsedNote, dict[str, str]]],
    deck_name: str,
    result: FileImportResult,
    cache: AnkiCache,
) -> list[tuple[ParsedNote, dict[str, str]]]:
    """Phase 1: Batch update existing notes (both QA and Cloze).

    Returns stale entries (note_id no longer in Anki) for re-creation.
    Also moves cards to the correct deck if they were moved between files.
    """
    if not existing_notes:
        return []

    existing_note_ids: list[int] = [
        parsed_note.note_id  # type: ignore[misc]
        for parsed_note, _ in existing_notes
        if parsed_note.note_id is not None
    ]

    # Use pre-fetched cache, or fetch on demand (single-file mode)
    if cache.notes:
        notes_by_id = {
            nid: cache.notes[nid] for nid in existing_note_ids if nid in cache.notes
        }
    else:
        try:
            notes_by_id = _fetch_notes_by_id(existing_note_ids)
        except Exception as e:
            for parsed_note, _ in existing_notes:
                result.errors.append(
                    f"Note {parsed_note.note_id} ({_note_identifier(parsed_note)}): {e}"
                )
            return []

    # Safety check: Ensure note types in markdown match note types in Anki
    for parsed_note, _ in existing_notes:
        if parsed_note.note_id is None:
            raise ValueError(
                "Internal error: note_id is None for note in existing_notes list"
            )
        note_info = notes_by_id.get(parsed_note.note_id)
        anki_note_type = note_info.model_name if note_info else None
        if anki_note_type and anki_note_type != parsed_note.note_type:
            raise ValueError(
                f"Note type mismatch for note {parsed_note.note_id} "
                f"({_note_identifier(parsed_note)}): "
                f"Markdown specifies '{parsed_note.note_type}' "
                f"but Anki has '{anki_note_type}'. "
                f"AnkiConnect does not support changing note types. "
                f"Please manually change the note type in Anki "
                f"or delete the old note_id HTML tag to re-create "
                f"the note."
            )

    # Move cards that are in the wrong deck
    all_card_ids = [cid for info in notes_by_id.values() for cid in info.card_ids]
    if all_card_ids:
        try:
            cards_info = [
                cache.cards[cid] for cid in all_card_ids if cid in cache.cards
            ]
            cards_to_move = [
                c["cardId"]
                for c in cards_info
                if c and c.get("deckName") and c["deckName"] != deck_name
            ]
            if cards_to_move:
                invoke("changeDeck", cards=cards_to_move, deck=deck_name)
                card_to_note_map = {
                    cid: info.note_id
                    for info in notes_by_id.values()
                    for cid in info.card_ids
                }
                card_to_deck_map = {c["cardId"]: c["deckName"] for c in cards_info if c}
                moved_notes = set()
                for cid in cards_to_move:
                    nid = card_to_note_map.get(cid)
                    if nid and nid not in moved_notes:
                        moved_notes.add(nid)
                        logger.info(
                            f"  Moved note {nid} from "
                            f"'{card_to_deck_map.get(cid, '?')}' "
                            f"to '{deck_name}'"
                        )
        except Exception as e:
            for parsed_note, _ in existing_notes:
                result.errors.append(
                    f"Note {parsed_note.note_id}: failed to move deck: {e}"
                )

    updates: list[dict] = []
    notes_to_update: list[ParsedNote] = []
    stale: list[tuple[ParsedNote, dict[str, str]]] = []

    for parsed_note, html_fields in existing_notes:
        if parsed_note.note_id is None:
            raise ValueError(
                "Internal error: note_id is None for note in existing_notes list"
            )
        note_info = notes_by_id.get(parsed_note.note_id)
        current = note_info.fields if note_info else {}
        if not current:
            logger.info(
                f"  Note {parsed_note.note_id} "
                f"({_note_identifier(parsed_note)}) "
                f"no longer in Anki, will re-create"
            )
            stale.append((parsed_note, html_fields))
            continue

        if all(current.get(name) == value for name, value in html_fields.items()):
            result.skipped += 1
            continue

        updates.append(
            {
                "action": "updateNoteFields",
                "params": {
                    "note": {
                        "id": parsed_note.note_id,
                        "fields": html_fields,
                    }
                },
            }
        )
        notes_to_update.append(parsed_note)

    if updates:
        try:
            multi_results = invoke("multi", actions=updates)
            for i, res in enumerate(multi_results):
                if res is None:
                    result.updated += 1
                else:
                    result.errors.append(
                        f"Note {notes_to_update[i].note_id} "
                        f"({_note_identifier(notes_to_update[i])})"
                        f": {res}"
                    )
        except Exception as e:
            for parsed_note in notes_to_update:
                result.errors.append(
                    f"Note {parsed_note.note_id} ({_note_identifier(parsed_note)}): {e}"
                )

    return stale


def _create_new_notes(
    new_notes: list[tuple[ParsedNote, dict[str, str]]],
    deck_name: str,
    result: FileImportResult,
) -> list[tuple[ParsedNote, int]]:
    """Phase 3: Batch create new notes (both QA and Cloze).

    Returns (parsed_note, note_id) pairs for ID writeback.
    """
    if not new_notes:
        return []

    create_actions = [
        {
            "action": "addNote",
            "params": {
                "note": {
                    "deckName": deck_name,
                    "modelName": parsed_note.note_type,
                    "fields": html_fields,
                    "options": {"allowDuplicate": False},
                }
            },
        }
        for parsed_note, html_fields in new_notes
    ]

    try:
        create_results = invoke("multi", actions=create_actions)
    except Exception as e:
        for parsed_note, _ in new_notes:
            result.errors.append(f"Note new ({_note_identifier(parsed_note)}): {e}")
        return []

    note_id_assignments: list[tuple[ParsedNote, int]] = []
    for i, note_id in enumerate(create_results):
        parsed_note, _ = new_notes[i]
        if note_id and isinstance(note_id, int):
            note_id_assignments.append((parsed_note, note_id))
            result.created += 1
        else:
            result.errors.append(
                f"Note new ({_note_identifier(parsed_note)}): {note_id}"
            )

    return note_id_assignments


def _delete_orphaned_notes(
    parsed_notes: list[ParsedNote],
    deck_name: str,
    result: FileImportResult,
    cache: AnkiCache,
    global_note_ids: set[int] | None = None,
) -> None:
    """Delete notes from Anki that are no longer in the markdown file.

    Must run BEFORE creating new notes, so newly created notes
    can't be mistaken for orphans.

    When global_note_ids is provided (collection mode),
    notes claimed by other files are excluded from deletion — those
    files will move the notes to the correct deck when processed.
    """
    md_note_ids = {n.note_id for n in parsed_notes if n.note_id is not None}

    anki_note_ids = cache.deck_note_ids.get(deck_name, set()).copy()
    if not anki_note_ids:
        return

    orphaned_notes = anki_note_ids - md_note_ids
    if global_note_ids:
        orphaned_notes -= global_note_ids

    if orphaned_notes:
        # Get note types for proper logging
        notes_info = invoke("notesInfo", notes=list(orphaned_notes))
        note_id_to_type = {
            note["noteId"]: note["modelName"] for note in notes_info if note
        }

        # Build note_id -> card_ids mapping for logging
        note_to_cards: dict[int, list[int]] = {}
        for cid, card in cache.cards.items():
            if card["note"] in orphaned_notes and card["deckName"] == deck_name:
                note_to_cards.setdefault(card["note"], []).append(cid)

        invoke("deleteNotes", notes=list(orphaned_notes))
        result.deleted += len(orphaned_notes)

        for nid, cids in note_to_cards.items():
            note_type = note_id_to_type.get(nid, "unknown")
            cid_str = ", ".join(str(c) for c in cids)
            logger.info(
                f"  Deleted {note_type} note {nid}"
                f"{f' (cards {cid_str})' if cid_str else ''}"
                f" from Anki"
            )


def import_file(
    file_path: Path,
    deck_name: str | None = None,
    only_add_new: bool = False,
    deck_names_and_ids: dict[str, int] | None = None,
    global_note_ids: set[int] | None = None,
    pre_parsed_notes: list[ParsedNote] | None = None,
    converter: MarkdownToHTML | None = None,
    cache: AnkiCache | None = None,
) -> FileImportResult:
    """Import all notes from a markdown file into Anki.

    When global_note_ids is provided, orphan deletion will not remove
    notes that are tracked by other files in the collection — they
    will be moved to the correct deck instead.

    When pre_parsed_notes/converter/cache are provided (collection
    mode), uses pre-fetched data to avoid redundant file reads and
    API calls.
    """
    # Auto-detect deck name
    if deck_name is None:
        if deck_names_and_ids is None:
            deck_names_and_ids = invoke("deckNamesAndIds")
        if deck_names_and_ids is None:
            raise ValueError(
                "Internal error: deck_names_and_ids required when deck_name is None"
            )
        id_to_name = {v: k for k, v in deck_names_and_ids.items()}

        file_deck_id = parse_deck_id(file_path)
        if file_deck_id and file_deck_id in id_to_name:
            deck_name = id_to_name[file_deck_id]
            logger.debug(f"Resolved deck by ID {file_deck_id}: {deck_name}")
        else:
            deck_name = file_path.stem.replace("__", "::")

            if deck_name not in deck_names_and_ids:
                new_deck_id = invoke("createDeck", deck=deck_name)
                write_deck_id_to_file(file_path, new_deck_id)
                deck_names_and_ids[deck_name] = new_deck_id
                logger.info(f"Created new deck '{deck_name}' (id: {new_deck_id})")
            elif not file_deck_id:
                existing_deck_id = deck_names_and_ids[deck_name]
                write_deck_id_to_file(file_path, existing_deck_id)
                logger.debug(f"Wrote deck_id {existing_deck_id} to {file_path.name}")

    parsed_notes = pre_parsed_notes or parse_markdown_file(file_path)
    if converter is None:
        converter = MarkdownToHTML()

    # Build cache on demand for single-file imports
    if cache is None:
        cache = AnkiCache()
        cache.cards, cache.deck_note_ids = _fetch_cards_cache()

    result = FileImportResult(
        file_path=file_path,
        total_notes=len(parsed_notes),
        updated=0,
        created=0,
        deleted=0,
        skipped=0,
    )

    # Phase 0: Classify notes and convert fields
    existing: list[tuple[ParsedNote, dict[str, str]]] = []
    new: list[tuple[ParsedNote, dict[str, str]]] = []

    for parsed_note in parsed_notes:
        validation_errors = validate_note(parsed_note)
        if validation_errors:
            for err in validation_errors:
                result.errors.append(
                    f"Note {parsed_note.note_id or 'new'} "
                    f"({_note_identifier(parsed_note)}): {err}"
                )
            continue

        try:
            html_fields = convert_fields_to_html(parsed_note.fields, converter)
        except Exception as e:
            result.errors.append(
                f"Note {parsed_note.note_id or 'new'} "
                f"({_note_identifier(parsed_note)}): {e}"
            )
            continue

        if parsed_note.note_id:
            if only_add_new:
                result.skipped += 1
                continue
            existing.append((parsed_note, html_fields))
        elif deck_name:
            new.append((parsed_note, html_fields))
        else:
            result.errors.append(
                f"Note new ({_note_identifier(parsed_note)}): No deck_name"
            )

    # Phase 1: Update existing notes (returns stale entries)
    stale = _import_existing_notes(existing, deck_name, result, cache)
    new.extend(stale)

    # Phase 2: Delete orphaned notes
    # (BEFORE creation so new notes aren't mistaken for orphans)
    if deck_name:
        _delete_orphaned_notes(
            parsed_notes,
            deck_name,
            result,
            cache,
            global_note_ids=global_note_ids,
        )

    # Phase 3: Create new notes (includes stale re-creations)
    id_assignments = _create_new_notes(new, deck_name, result)

    # Phase 4: Write IDs back to file
    _batch_write_ids_to_file(file_path, id_assignments)

    return result


def delete_untracked_notes(collection_dir: str) -> int:
    """Delete DeckOps notes from Anki decks that have no markdown file.

    Only affects notes with managed note types
    (DeckOpsQA, DeckOpsCloze, DeckOpsChoice).
    Other note types in the same deck are left untouched.
    Only targets decks whose deck_id does not match any file's deck_id
    in the collection directory.
    """
    collection_path = Path(collection_dir)

    md_deck_ids: set[int] = set()
    for md_file in collection_path.glob("*.md"):
        deck_id = parse_deck_id(md_file)
        if deck_id is not None:
            md_deck_ids.add(deck_id)

    deck_names_and_ids = invoke("deckNamesAndIds")

    # Single query to find all cards with supported note types
    query = " OR ".join(f"note:{nt}" for nt in SUPPORTED_NOTE_TYPES)
    all_card_ids = invoke("findCards", query=query)

    orphaned: list[tuple[str, int, list[int]]] = []
    if all_card_ids:
        cards_info = invoke("cardsInfo", cards=all_card_ids)
        deck_notes: dict[str, set[int]] = {}
        for card in cards_info:
            deck_notes.setdefault(card["deckName"], set()).add(card["note"])

        for deck_name, note_ids in deck_notes.items():
            deck_id = deck_names_and_ids.get(deck_name)
            if deck_id is None or deck_id in md_deck_ids:
                continue
            orphaned.append((deck_name, deck_id, list(note_ids)))

    if not orphaned:
        return 0

    logger.warning(
        "The following Anki decks with DeckOps notes inside "
        "have no matching markdown file:"
    )
    for deck_name, deck_id, note_ids in orphaned:
        logger.warning(
            f"  - '{deck_name}' (deck_id: {deck_id}, {len(note_ids)} managed notes)"
        )

    answer = (
        input(
            "Delete these managed notes from Anki "
            "(only DeckOps notes will be removed)? [y/N] "
        )
        .strip()
        .lower()
    )
    if answer != "y":
        logger.info("Skipped untracked note deletion.")
        return 0

    total_deleted = 0
    for deck_name, deck_id, note_ids in orphaned:
        invoke("deleteNotes", notes=note_ids)
        total_deleted += len(note_ids)
        logger.info(
            f"  Deleted {len(note_ids)} managed notes from "
            f"'{deck_name}' (deck_id: {deck_id})"
        )

    return total_deleted


def import_collection(
    collection_dir: str,
    only_add_new: bool = False,
) -> list[FileImportResult]:
    """Import all markdown files in a directory back into Anki."""
    collection_path = Path(collection_dir)
    deck_names_and_ids = invoke("deckNamesAndIds")
    results = []

    # Pre-parse all files to collect globally tracked IDs.
    # This prevents orphan deletion from removing notes that were
    # moved between files — the claiming file will move them instead.
    md_files = sorted(collection_path.glob("*.md"))
    global_note_ids: set[int] = set()
    note_id_sources: dict[int, str] = {}
    deck_id_sources: dict[int, str] = {}
    duplicates: list[str] = []
    duplicate_ids: set[int] = set()
    file_parsed_notes: dict[Path, list[ParsedNote]] = {}

    for md_file in md_files:
        content = md_file.read_text(encoding="utf-8")
        deck_id, remaining = extract_deck_id(content)
        blocks = remaining.split(NOTE_SEPARATOR)
        parsed_notes = [parse_note_block(block) for block in blocks if block.strip()]
        file_parsed_notes[md_file] = parsed_notes

        if deck_id is not None:
            if deck_id in deck_id_sources:
                duplicates.append(
                    f"Duplicate deck_id {deck_id} found in "
                    f"'{md_file.name}' and "
                    f"'{deck_id_sources[deck_id]}'"
                )
                duplicate_ids.add(deck_id)
            else:
                deck_id_sources[deck_id] = md_file.name

        for parsed_note in parsed_notes:
            if parsed_note.note_id is not None:
                if parsed_note.note_id in note_id_sources:
                    duplicates.append(
                        f"Duplicate note_id "
                        f"{parsed_note.note_id} "
                        f"found in '{md_file.name}' and "
                        f"'{note_id_sources[parsed_note.note_id]}'"
                    )
                    duplicate_ids.add(parsed_note.note_id)
                else:
                    note_id_sources[parsed_note.note_id] = md_file.name
                global_note_ids.add(parsed_note.note_id)

    if duplicates:
        for dup in duplicates:
            logger.error(dup)
        ids_str = ", ".join(str(i) for i in sorted(duplicate_ids))
        raise ValueError(
            f"Aborting import: {len(duplicate_ids)} duplicate "
            f"ID(s) found across files: {ids_str}. "
            f"Each deck/note ID must appear in exactly one file."
        )

    # Batch API calls: fetch all note info and card info upfront
    # instead of per-file API calls (reduces O(N) calls to O(1)).
    converter = MarkdownToHTML()
    cache = AnkiCache()

    if global_note_ids:
        cache.notes = _fetch_notes_by_id(list(global_note_ids))

    cache.cards, cache.deck_note_ids = _fetch_cards_cache()

    for md_file in md_files:
        logger.info(f"Processing {md_file.name}...")
        result = import_file(
            md_file,
            only_add_new=only_add_new,
            deck_names_and_ids=deck_names_and_ids,
            global_note_ids=global_note_ids,
            pre_parsed_notes=file_parsed_notes[md_file],
            converter=converter,
            cache=cache,
        )
        results.append(result)

        logger.info(
            f"  Updated: {result.updated}, Created: {result.created}"
            f", Deleted: {result.deleted}"
            f", Skipped: {result.skipped}"
            f", Errors: {len(result.errors)}"
        )

        for error in result.errors:
            logger.error(f"  {error}")

    return results
