"""Import Markdown files back into Anki.

Architecture:
  AnkiState   – all Anki-side data, fetched once (shared from anki_client)
  FileState   – one markdown file, read once
  _sync_file  – single engine: classify → update → delete → create
  _flush_writes – deferred file I/O (one write per file, at the end)
"""

import logging
from dataclasses import dataclass, field
from pathlib import Path

from deckops.anki_client import AnkiState, invoke
from deckops.config import NOTE_SEPARATOR
from deckops.markdown_converter import MarkdownToHTML
from deckops.markdown_helpers import (
    ParsedNote,
    extract_deck_id,
    note_identifier,
    parse_note_block,
    validate_note,
)

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------


@dataclass
class FileImportResult:
    """Result of importing a single file."""

    file_path: Path
    deck_name: str
    total_notes: int
    updated: int
    created: int
    deleted: int
    moved: int
    skipped: int
    errors: list[str] = field(default_factory=list)


@dataclass
class UntrackedDeck:
    """An Anki deck with DeckOps notes but no matching markdown file."""

    deck_name: str
    deck_id: int
    note_ids: list[int]


@dataclass
class ImportSummary:
    """Aggregate result of a full collection import."""

    file_results: list[FileImportResult]
    untracked_decks: list[UntrackedDeck]


@dataclass
class FileState:
    """All data parsed from one markdown file in a single read."""

    file_path: Path
    raw_content: str
    deck_id: int | None
    parsed_notes: list[ParsedNote]

    @staticmethod
    def from_file(file_path: Path) -> "FileState":
        raw_content = file_path.read_text(encoding="utf-8")
        deck_id, remaining = extract_deck_id(raw_content)
        blocks = remaining.split(NOTE_SEPARATOR)
        parsed_notes = [parse_note_block(block) for block in blocks if block.strip()]
        return FileState(
            file_path=file_path,
            raw_content=raw_content,
            deck_id=deck_id,
            parsed_notes=parsed_notes,
        )


@dataclass
class _PendingWrite:
    """Deferred file modification, applied after all Anki mutations."""

    file_path: Path
    raw_content: str
    deck_id_to_write: int | None
    id_assignments: list[tuple[ParsedNote, int]]


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _extract_fields(raw_note: dict) -> dict[str, str]:
    """Extract {field_name: value} from a raw AnkiConnect note dict."""
    return {name: data["value"] for name, data in raw_note["fields"].items()}


def convert_fields_to_html(
    fields: dict[str, str],
    converter: MarkdownToHTML,
) -> dict[str, str]:
    """Convert all field values from markdown to HTML."""
    return {name: converter.convert(content) for name, content in fields.items()}


def _validate_no_duplicate_first_lines(
    file_path: Path,
    id_assignments: list[tuple[ParsedNote, int]],
) -> None:
    """Raise if new notes share a first line (would break text-based ID insertion)."""
    first_lines: dict[str, list[str]] = {}
    for parsed_note, _ in id_assignments:
        if parsed_note.note_id is not None:
            continue
        first_line = parsed_note.raw_content.strip().split("\n")[0]
        first_lines.setdefault(first_line, []).append(note_identifier(parsed_note))

    duplicates = {
        line: ids for line, ids in first_lines.items() if len(ids) > 1
    }
    if duplicates:
        msg = f"ERROR: Duplicate first lines detected in {file_path.name}:\n"
        for first_line, ids in duplicates.items():
            msg += f"  '{first_line[:60]}...' in notes: {', '.join(ids)}\n"
        msg += (
            "Cannot safely assign IDs. "
            "Please ensure each note has a unique first line."
        )
        raise ValueError(msg)


def _flush_writes(writes: list[_PendingWrite]) -> None:
    """Apply all deferred file modifications (one write per file)."""
    for w in writes:
        if w.deck_id_to_write is None and not w.id_assignments:
            continue

        content = w.raw_content

        # 1. Insert or replace deck_id
        if w.deck_id_to_write is not None:
            _, remaining = extract_deck_id(content)
            content = f"<!-- deck_id: {w.deck_id_to_write} -->\n" + remaining

        # 2. Insert note_ids for new / stale notes
        if w.id_assignments:
            _validate_no_duplicate_first_lines(w.file_path, w.id_assignments)

            for parsed_note, id_value in w.id_assignments:
                new_id_comment = f"<!-- note_id: {id_value} -->"
                if parsed_note.note_id is not None:
                    old_id_comment = f"<!-- note_id: {parsed_note.note_id} -->"
                    content = content.replace(old_id_comment, new_id_comment, 1)
                else:
                    first_line = parsed_note.raw_content.strip().split("\n")[0]
                    content = content.replace(
                        first_line, new_id_comment + "\n" + first_line, 1
                    )

        w.file_path.write_text(content, encoding="utf-8")


# ---------------------------------------------------------------------------
# Sync engine
# ---------------------------------------------------------------------------


def _sync_file(
    fs: FileState,
    anki: AnkiState,
    converter: MarkdownToHTML,
    only_add_new: bool = False,
    global_note_ids: set[int] | None = None,
) -> tuple[FileImportResult, _PendingWrite]:
    """Synchronize one markdown file to Anki.

    Phases:
      0. Resolve deck (by deck_id or filename)
      1. Classify notes (existing vs new) + convert md → html
      2. Update existing notes + move cards between decks
      3. Delete orphaned notes
      4. Create new notes (including stale re-creations)
      5. Return result + pending file write
    """
    # ---- Phase 0: Resolve deck ----
    deck_id_to_write: int | None = None

    if fs.deck_id and fs.deck_id in anki.id_to_deck_name:
        deck_name = anki.id_to_deck_name[fs.deck_id]
        logger.debug(f"Resolved deck by ID {fs.deck_id}: {deck_name}")
    else:
        deck_name = fs.file_path.stem.replace("__", "::")

        if deck_name not in anki.deck_names_and_ids:
            new_deck_id = invoke("createDeck", deck=deck_name)
            anki.deck_names_and_ids[deck_name] = new_deck_id
            anki.id_to_deck_name[new_deck_id] = deck_name
            deck_id_to_write = new_deck_id
            logger.info(f"Created new deck '{deck_name}' (id: {new_deck_id})")
        elif not fs.deck_id:
            deck_id_to_write = anki.deck_names_and_ids[deck_name]
            logger.debug(
                f"Wrote deck_id {deck_id_to_write} to {fs.file_path.name}"
            )

    result = FileImportResult(
        file_path=fs.file_path,
        deck_name=deck_name,
        total_notes=len(fs.parsed_notes),
        updated=0,
        created=0,
        deleted=0,
        moved=0,
        skipped=0,
    )

    # ---- Phase 1: Classify + convert ----
    existing: list[tuple[ParsedNote, dict[str, str]]] = []
    new: list[tuple[ParsedNote, dict[str, str]]] = []

    for note in fs.parsed_notes:
        validation_errors = validate_note(note)
        if validation_errors:
            for err in validation_errors:
                result.errors.append(
                    f"Note {note.note_id or 'new'} "
                    f"({note_identifier(note)}): {err}"
                )
            continue

        try:
            html_fields = convert_fields_to_html(note.fields, converter)
        except Exception as e:
            result.errors.append(
                f"Note {note.note_id or 'new'} "
                f"({note_identifier(note)}): {e}"
            )
            continue

        if note.note_id:
            if only_add_new:
                result.skipped += 1
            else:
                existing.append((note, html_fields))
        else:
            new.append((note, html_fields))

    # ---- Phase 2: Update existing + move cards ----

    # Safety check: note type mismatches
    for note, _ in existing:
        raw = anki.notes.get(note.note_id)  # type: ignore[arg-type]
        if raw and raw.get("modelName") != note.note_type:
            raise ValueError(
                f"Note type mismatch for note {note.note_id} "
                f"({note_identifier(note)}): "
                f"Markdown specifies '{note.note_type}' "
                f"but Anki has '{raw.get('modelName')}'. "
                f"AnkiConnect does not support changing note types. "
                f"Please manually change the note type in Anki "
                f"or delete the old note_id HTML tag to re-create the note."
            )

    # Move cards that are in the wrong deck
    cards_to_move: list[int] = []
    for note, _ in existing:
        raw = anki.notes.get(note.note_id)  # type: ignore[arg-type]
        if not raw:
            continue
        for cid in raw.get("cards", []):
            card = anki.cards.get(cid)
            if card and card.get("deckName") != deck_name:
                cards_to_move.append(cid)

    if cards_to_move:
        try:
            invoke("changeDeck", cards=cards_to_move, deck=deck_name)
            moved_notes: set[int] = set()
            for cid in cards_to_move:
                card = anki.cards.get(cid)
                if card:
                    nid = card["note"]
                    if nid not in moved_notes:
                        moved_notes.add(nid)
                        logger.info(
                            f"  Moved note {nid} from "
                            f"'{card['deckName']}' to '{deck_name}'"
                        )
            result.moved += len(moved_notes)
        except Exception as e:
            for note, _ in existing:
                result.errors.append(
                    f"Note {note.note_id}: failed to move deck: {e}"
                )

    # Build update batch
    stale: list[tuple[ParsedNote, dict[str, str]]] = []
    updates: list[dict] = []
    update_notes: list[ParsedNote] = []

    for note, html_fields in existing:
        raw = anki.notes.get(note.note_id)  # type: ignore[arg-type]
        if not raw or not raw.get("fields"):
            logger.info(
                f"  Note {note.note_id} ({note_identifier(note)}) "
                f"no longer in Anki, will re-create"
            )
            stale.append((note, html_fields))
            continue

        anki_fields = _extract_fields(raw)
        if all(anki_fields.get(k) == v for k, v in html_fields.items()):
            result.skipped += 1
            continue

        updates.append(
            {
                "action": "updateNoteFields",
                "params": {
                    "note": {"id": note.note_id, "fields": html_fields}
                },
            }
        )
        update_notes.append(note)

    if updates:
        try:
            multi_results = invoke("multi", actions=updates)
            for i, res in enumerate(multi_results):
                if res is None:
                    result.updated += 1
                else:
                    result.errors.append(
                        f"Note {update_notes[i].note_id} "
                        f"({note_identifier(update_notes[i])}): {res}"
                    )
        except Exception as e:
            for note in update_notes:
                result.errors.append(
                    f"Note {note.note_id} ({note_identifier(note)}): {e}"
                )

    new.extend(stale)

    # ---- Phase 3: Delete orphaned notes ----
    md_note_ids = {n.note_id for n in fs.parsed_notes if n.note_id is not None}
    anki_deck_note_ids = anki.deck_note_ids.get(deck_name, set()).copy()
    orphaned = anki_deck_note_ids - md_note_ids
    if global_note_ids:
        orphaned -= global_note_ids

    if orphaned:
        for nid in orphaned:
            raw = anki.notes.get(nid)
            model = raw.get("modelName", "unknown") if raw else "unknown"
            cids = [
                cid
                for cid, c in anki.cards.items()
                if c["note"] == nid and c["deckName"] == deck_name
            ]
            cid_str = ", ".join(str(c) for c in cids)
            logger.info(
                f"  Deleted {model} note {nid}"
                f"{f' (cards {cid_str})' if cid_str else ''}"
                f" from Anki"
            )
        invoke("deleteNotes", notes=list(orphaned))
        result.deleted += len(orphaned)

    # ---- Phase 4: Create new notes ----
    id_assignments: list[tuple[ParsedNote, int]] = []

    if new:
        create_actions = [
            {
                "action": "addNote",
                "params": {
                    "note": {
                        "deckName": deck_name,
                        "modelName": note.note_type,
                        "fields": html_fields,
                        "options": {"allowDuplicate": False},
                    }
                },
            }
            for note, html_fields in new
        ]

        try:
            create_results = invoke("multi", actions=create_actions)
            for i, note_id in enumerate(create_results):
                note, _ = new[i]
                if note_id and isinstance(note_id, int):
                    id_assignments.append((note, note_id))
                    result.created += 1
                else:
                    result.errors.append(
                        f"Note new ({note_identifier(note)}): {note_id}"
                    )
        except Exception as e:
            for note, _ in new:
                result.errors.append(
                    f"Note new ({note_identifier(note)}): {e}"
                )

    # ---- Phase 5: Return result + pending write ----
    pending = _PendingWrite(
        file_path=fs.file_path,
        raw_content=fs.raw_content,
        deck_id_to_write=deck_id_to_write,
        id_assignments=id_assignments,
    )
    return result, pending


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def import_file(
    file_path: Path,
    only_add_new: bool = False,
) -> FileImportResult:
    """Import a single markdown file into Anki."""
    fs = FileState.from_file(file_path)
    anki = AnkiState.fetch()
    converter = MarkdownToHTML()

    result, pending = _sync_file(fs, anki, converter, only_add_new=only_add_new)
    _flush_writes([pending])

    return result


def import_collection(
    collection_dir: str,
    only_add_new: bool = False,
) -> ImportSummary:
    """Import all markdown files in a directory back into Anki.

    Single pass:
      1. Parse all files (one read each)
      2. Cross-file validation (duplicate IDs)
      3. Fetch all Anki state (3-4 API calls)
      4. Sync each file
      5. Flush all file writes
      6. Detect untracked decks (returned for CLI confirmation)
    """
    collection_path = Path(collection_dir)
    md_files = sorted(collection_path.glob("*.md"))

    # Phase 1: Parse all files
    file_states = [FileState.from_file(f) for f in md_files]

    # Phase 2: Cross-file validation
    global_note_ids: set[int] = set()
    note_id_sources: dict[int, str] = {}
    deck_id_sources: dict[int, str] = {}
    duplicates: list[str] = []
    duplicate_ids: set[int] = set()

    md_deck_ids: set[int] = set()

    for fs in file_states:
        if fs.deck_id is not None:
            md_deck_ids.add(fs.deck_id)
            if fs.deck_id in deck_id_sources:
                duplicates.append(
                    f"Duplicate deck_id {fs.deck_id} found in "
                    f"'{fs.file_path.name}' and "
                    f"'{deck_id_sources[fs.deck_id]}'"
                )
                duplicate_ids.add(fs.deck_id)
            else:
                deck_id_sources[fs.deck_id] = fs.file_path.name

        for note in fs.parsed_notes:
            if note.note_id is not None:
                if note.note_id in note_id_sources:
                    duplicates.append(
                        f"Duplicate note_id {note.note_id} "
                        f"found in '{fs.file_path.name}' and "
                        f"'{note_id_sources[note.note_id]}'"
                    )
                    duplicate_ids.add(note.note_id)
                else:
                    note_id_sources[note.note_id] = fs.file_path.name
                global_note_ids.add(note.note_id)

    if duplicates:
        for dup in duplicates:
            logger.error(dup)
        ids_str = ", ".join(str(i) for i in sorted(duplicate_ids))
        raise ValueError(
            f"Aborting import: {len(duplicate_ids)} duplicate "
            f"ID(s) found across files: {ids_str}. "
            f"Each deck/note ID must appear in exactly one file."
        )

    # Phase 3: Fetch all Anki state
    anki = AnkiState.fetch()
    converter = MarkdownToHTML()

    # Phase 4: Sync each file
    results: list[FileImportResult] = []
    pending_writes: list[_PendingWrite] = []

    for fs in file_states:
        logger.info(f"Processing {fs.file_path.name}...")
        file_result, pending = _sync_file(
            fs,
            anki,
            converter,
            only_add_new=only_add_new,
            global_note_ids=global_note_ids,
        )
        results.append(file_result)
        pending_writes.append(pending)

        logger.info(
            f"  Updated: {file_result.updated}, Created: {file_result.created}"
            f", Deleted: {file_result.deleted}, Moved: {file_result.moved}"
            f", Skipped: {file_result.skipped}"
            f", Errors: {len(file_result.errors)}"
        )
        for error in file_result.errors:
            logger.error(f"  {error}")

    # Phase 5: Flush all file writes
    _flush_writes(pending_writes)

    # Phase 6: Detect untracked decks
    untracked_decks: list[UntrackedDeck] = []
    for deck_name, note_ids in anki.deck_note_ids.items():
        deck_id = anki.deck_names_and_ids.get(deck_name)
        if deck_id is None or deck_id in md_deck_ids:
            continue
        untracked_decks.append(UntrackedDeck(deck_name, deck_id, list(note_ids)))

    return ImportSummary(
        file_results=results,
        untracked_decks=untracked_decks,
    )
